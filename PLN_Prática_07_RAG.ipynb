{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuarteVn/RAG-Simples/blob/main/PLN_Pr%C3%A1tica_07_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zps-hp8P-qqG"
      },
      "source": [
        "# Processamento de Linguagem Natural e IA Generativa\n",
        "**Prática Aula 07: Retrieval Augmented Generation**\n",
        "\n",
        "- Implementação do pipeline de um sistema RAG Simples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPeCK_4R_CKJ"
      },
      "source": [
        "### PARTE 1: Instalação das dependências necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkJBOFWo_ZPa"
      },
      "outputs": [],
      "source": [
        "# transformers → Biblioteca da Hugging Face com modelos pré-treinados\n",
        "# torch → framework de deep learning para redes neurais\n",
        "# faiss-cpu → Para armazenamento e busca vetorial\n",
        "# gradio → Criar interfaces web interativas rapidamente\n",
        "# sentence-transformers → embeddings de sentenças\n",
        "\n",
        "!pip install transformers torch faiss-cpu gradio sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ciNn0c_bss"
      },
      "source": [
        "### PARTE 2: Importações e configuração básica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WigU1BI_deR",
        "outputId": "e4b36801-83fe-4089-9f00-194faa56deac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dispositivo em uso: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "# AutoTokenizer: carrega automaticamente o tokenizador correto para o modelo\n",
        "# T5ForConditionalGeneration: modelo T5 para tarefas de geração de texto\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# embeddings para busca semântica\n",
        "\n",
        "# Verificar se GPU está disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Dispositivo em uso: {device}\")\n",
        "\n",
        "# Configuração dos modelos\n",
        "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "LLM_MODEL = \"google/flan-t5-base\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vCQy16L_iMl"
      },
      "source": [
        "### PARTE 3: Carregamento dos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R59rYDa7_nS_",
        "outputId": "7fe5b16e-d447-4224-ae09-1d4b9af7c527"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
        "model = T5ForConditionalGeneration.from_pretrained(LLM_MODEL)\n",
        "\n",
        "# Mover para GPU se disponível\n",
        "if device.type == \"cuda\":\n",
        "    model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PkI4R__A3hO"
      },
      "source": [
        "### PARTE 4 - Cria uma base de dados de exemplo para demonstração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqVEoXmyApGL",
        "outputId": "f7ffce30-3eaf-4a17-ed75-e26a454de184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Criando base de dados de exemplo...\n",
            "Erro ao criar base de exemplo: name 'criar_embedding' is not defined\n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = \"DB/\"\n",
        "\n",
        "def criar_base_exemplo():\n",
        "    \"\"\"\n",
        "    Cria uma base de dados de exemplo para demonstração\n",
        "    \"\"\"\n",
        "    print(\"Criando base de dados de exemplo...\")\n",
        "\n",
        "    # Dados de exemplo\n",
        "    documentos_exemplo = [\n",
        "        {\n",
        "            \"text\": \"Células-tronco são células com capacidade de autorenovação e diferenciação em vários tipos celulares. Podem ser embrionárias, adultas ou induzidas.\",\n",
        "            \"source\": \"Manual de Biologia Celular\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"A dengue é uma doença viral transmitida pelo mosquito Aedes aegypti. Sintomas incluem febre alta, dor de cabeça e dores musculares.\",\n",
        "            \"source\": \"Guia de Doenças Tropicais\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Vacinas funcionam estimulando o sistema imunológico a reconhecer patógenos específicos sem causar a doença.\",\n",
        "            \"source\": \"Fundamentos da Imunologia\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Epidemiologia estuda a distribuição e fatores determinantes de doenças em populações específicas.\",\n",
        "            \"source\": \"Princípios de Epidemiologia\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Criar diretório\n",
        "        caminho_exemplo = os.path.join(BASE_DIR, \"base_biologia\")\n",
        "        os.makedirs(caminho_exemplo, exist_ok=True)\n",
        "\n",
        "        # Gerar embeddings\n",
        "        embeddings = []\n",
        "        for doc in documentos_exemplo:\n",
        "            embedding = criar_embedding(doc[\"text\"])\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        # Criar índice FAISS\n",
        "        embeddings_array = np.array(embeddings).astype('float32')\n",
        "        dimensao = embeddings_array.shape[1]\n",
        "        indice = faiss.IndexFlatL2(dimensao)\n",
        "        indice.add(embeddings_array)\n",
        "\n",
        "        # Salvar arquivos\n",
        "        faiss.write_index(indice, os.path.join(caminho_exemplo, \"faiss_index.index\"))\n",
        "\n",
        "        with open(os.path.join(caminho_exemplo, \"chunk_metadata.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(documentos_exemplo, f)\n",
        "\n",
        "        print(\"Base de exemplo criada com sucesso!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao criar base de exemplo: {e}\")\n",
        "\n",
        "criar_base_exemplo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny3kF233_uPQ"
      },
      "source": [
        "### PARTE 5: Funções principais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYs9UxfJ_tWm"
      },
      "outputs": [],
      "source": [
        "def criar_embedding(texto: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Converte texto em vetor numérico (embedding)\n",
        "    \"\"\"\n",
        "    embedding = embedding_model.encode(texto, convert_to_tensor=False)\n",
        "    return embedding.tolist()\n",
        "\n",
        "def buscar_documentos_relevantes(pergunta: str, indice_faiss, lista_documentos, k=3):\n",
        "    \"\"\"\n",
        "    Busca os documentos mais similares à pergunta usando BD Vetorial FAISS\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Buscando documentos para: {pergunta[:50]}...\")\n",
        "\n",
        "        # Verificar se há documentos\n",
        "        if not lista_documentos:\n",
        "            print(\"Erro: Lista de documentos vazia\")\n",
        "            return []\n",
        "\n",
        "        # Criar embedding da pergunta\n",
        "        embedding_pergunta = criar_embedding(pergunta)\n",
        "        if not embedding_pergunta:\n",
        "            print(\"Erro: Não foi possível criar embedding da pergunta\")\n",
        "            return []\n",
        "\n",
        "        # Converter para formato FAISS\n",
        "        vetor_pergunta = np.array(embedding_pergunta).astype('float32').reshape(1, -1)\n",
        "        print(f\"Vetor criado com dimensão: {vetor_pergunta.shape}\")\n",
        "\n",
        "        # Ajustar k se necessário\n",
        "        k_ajustado = min(k, len(lista_documentos), indice_faiss.ntotal)\n",
        "        print(f\"Buscando {k_ajustado} documentos de {len(lista_documentos)} disponíveis\")\n",
        "\n",
        "        # Buscar documentos similares\n",
        "        distancias, indices = indice_faiss.search(vetor_pergunta, k_ajustado)\n",
        "\n",
        "        print(f\"Distâncias encontradas: {distancias[0]}\")\n",
        "        print(f\"Índices encontrados: {indices[0]}\")\n",
        "\n",
        "        # Verificar se encontrou resultados válidos\n",
        "        if len(indices[0]) == 0:\n",
        "            print(\"Nenhum índice retornado pela busca\")\n",
        "            return lista_documentos[:k_ajustado]  # Retorna primeiros documentos como fallback\n",
        "\n",
        "        # Retornar documentos encontrados\n",
        "        documentos_encontrados = []\n",
        "        for i in indices[0]:\n",
        "            if 0 <= i < len(lista_documentos):\n",
        "                documentos_encontrados.append(lista_documentos[i])\n",
        "                print(f\"Documento {i}: {lista_documentos[i]['text'][:100]}...\")\n",
        "\n",
        "        if not documentos_encontrados:\n",
        "            print(\"Nenhum documento válido encontrado, usando fallback\")\n",
        "            return lista_documentos[:k_ajustado]\n",
        "\n",
        "        print(f\"Retornando {len(documentos_encontrados)} documentos\")\n",
        "        return documentos_encontrados\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na busca: {e}\")\n",
        "        # Fallback: retorna primeiros documentos\n",
        "        return lista_documentos[:min(k, len(lista_documentos))]\n",
        "\n",
        "\n",
        "def gerar_resposta(pergunta: str, documentos_contexto: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Gera resposta com base no contexto fornecido\n",
        "    \"\"\"\n",
        "    # Preparar o contexto\n",
        "    contexto = \"\\n\\n\".join([doc[\"text\"] for doc in documentos_contexto])\n",
        "\n",
        "    # Criar prompt estruturado\n",
        "    prompt = f\"\"\"Com base nas informações fornecidas, responda à pergunta em português brasileiro.\n",
        "\n",
        "INFORMAÇÕES:\n",
        "{contexto}\n",
        "\n",
        "PERGUNTA: {pergunta}\n",
        "\n",
        "RESPOSTA:\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Tokenizar entrada\n",
        "        inputs = tokenizer.encode(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Mover para GPU se necessário\n",
        "        if device.type == \"cuda\":\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "        # Gerar resposta\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=200,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        # Decodificar resposta\n",
        "        resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Limpar resposta (remover prompt)\n",
        "        if \"RESPOSTA:\" in resposta:\n",
        "            resposta = resposta.split(\"RESPOSTA:\")[-1].strip()\n",
        "\n",
        "        # Adicionar fontes\n",
        "        fontes = [f\"- {doc['source']}\" for doc in documentos_contexto]\n",
        "        if fontes:\n",
        "            resposta += \"\\n\\nFontes:\\n\" + \"\\n\".join(fontes)\n",
        "\n",
        "        return resposta\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na geração: {e}\")\n",
        "        return f\"Não foi possível gerar uma resposta adequada. Erro: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6bBGiUR_-RM"
      },
      "source": [
        "### PARTE 6: Funções de gerenciamento de bases de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeq8DRfXAAa-"
      },
      "outputs": [],
      "source": [
        "def listar_bases_disponiveis():\n",
        "    \"\"\"\n",
        "    Lista todas as bases FAISS disponíveis no diretório\n",
        "    \"\"\"\n",
        "    bases = []\n",
        "    try:\n",
        "        for item in os.listdir(BASE_DIR):\n",
        "            caminho_base = os.path.join(BASE_DIR, item)\n",
        "            if os.path.isdir(caminho_base):\n",
        "                arquivo_indice = os.path.join(caminho_base, \"faiss_index.index\")\n",
        "                arquivo_metadados = os.path.join(caminho_base, \"chunk_metadata.pkl\")\n",
        "\n",
        "                if os.path.exists(arquivo_indice) and os.path.exists(arquivo_metadados):\n",
        "                    bases.append(item)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return bases\n",
        "\n",
        "def carregar_base_faiss(nome_base):\n",
        "    \"\"\"\n",
        "    Carrega uma base FAISS específica\n",
        "    \"\"\"\n",
        "    if not nome_base:\n",
        "        raise ValueError(\"Nome da base não fornecido\")\n",
        "\n",
        "    caminho_indice = os.path.join(BASE_DIR, nome_base, \"faiss_index.index\")\n",
        "    caminho_metadados = os.path.join(BASE_DIR, nome_base, \"chunk_metadata.pkl\")\n",
        "\n",
        "    # Verificar se existem arquivos\n",
        "    if not os.path.exists(caminho_indice):\n",
        "        raise FileNotFoundError(f\"Índice não encontrado: {caminho_indice}\")\n",
        "\n",
        "    if not os.path.exists(caminho_metadados):\n",
        "        raise FileNotFoundError(f\"Metadados não encontrados: {caminho_metadados}\")\n",
        "\n",
        "    # Carregar arquivos\n",
        "    indice = faiss.read_index(caminho_indice)\n",
        "\n",
        "    with open(caminho_metadados, \"rb\") as f:\n",
        "        documentos = pickle.load(f)\n",
        "\n",
        "    print(f\"Base carregada: {nome_base} ({len(documentos)} documentos)\")\n",
        "    return indice, documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sYiW0SLAIkq"
      },
      "source": [
        "### PARTE 7: Função principal do chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-Xum0FSDAIw"
      },
      "outputs": [],
      "source": [
        "def chat_principal(mensagem, historico, escolha_base, escolha_modelo):\n",
        "    \"\"\"\n",
        "    Função principal que processa as mensagens do chat\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validações básicas\n",
        "        if not mensagem.strip():\n",
        "            return \"Por favor, digite uma pergunta.\"\n",
        "\n",
        "        if not escolha_base:\n",
        "            return \"Por favor, selecione uma base de conhecimento primeiro.\"\n",
        "\n",
        "        # Carregar base de dados\n",
        "        indice, documentos = carregar_base_faiss(escolha_base)\n",
        "\n",
        "        # Buscar documentos relevantes\n",
        "        documentos_relevantes = buscar_documentos_relevantes(mensagem, indice, documentos)\n",
        "\n",
        "        if not documentos_relevantes:\n",
        "            return \"Não foram encontradas informações relevantes na base selecionada.\"\n",
        "\n",
        "        # Gerar resposta\n",
        "        resposta = gerar_resposta(mensagem, documentos_relevantes)\n",
        "\n",
        "        return resposta\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Erro ao processar pergunta: {str(e)}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn-y3Dv9Dt0w"
      },
      "source": [
        "### PARTE 8: Interface do ChatBot com Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5J_Fez7DEzsO",
        "outputId": "0e431cea-cab6-444c-b4d2-94f444a72800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando PrimeiroRAG...\n",
            "Bases disponíveis: 2\n",
            "Lançando interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://edeaddafb4481ee190.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://edeaddafb4481ee190.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base carregada: base_ciencia_dados (5 documentos)\n",
            "Buscando documentos para: Quais são as etapas de um pipeline?...\n",
            "Vetor criado com dimensão: (1, 384)\n",
            "Buscando 3 documentos de 5 disponíveis\n",
            "Distâncias encontradas: [18.22726 31.43446 35.32258]\n",
            "Índices encontrados: [1 4 3]\n",
            "Documento 1: Um pipeline de Ciência de Dados tem várias etapas: coleta dos dados, limpeza, análise exploratória (...\n",
            "Documento 4: Deploy é colocar o modelo para funcionar com dados reais. MLOps é o conjunto de boas práticas para m...\n",
            "Documento 3: As métricas servem para medir a qualidade dos modelos. Na classificação, usamos: precisão, recall, F...\n",
            "Retornando 3 documentos\n",
            "Base carregada: base_ciencia_dados (5 documentos)\n",
            "Buscando documentos para: O que é deploy?...\n",
            "Vetor criado com dimensão: (1, 384)\n",
            "Buscando 3 documentos de 5 disponíveis\n",
            "Distâncias encontradas: [21.565866 31.578342 35.92127 ]\n",
            "Índices encontrados: [4 1 0]\n",
            "Documento 4: Deploy é colocar o modelo para funcionar com dados reais. MLOps é o conjunto de boas práticas para m...\n",
            "Documento 1: Um pipeline de Ciência de Dados tem várias etapas: coleta dos dados, limpeza, análise exploratória (...\n",
            "Documento 0: Ciência de Dados é a área que usa estatística, programação e conhecimento de negócio para analisar d...\n",
            "Retornando 3 documentos\n",
            "Base carregada: base_ciencia_dados (5 documentos)\n",
            "Buscando documentos para: Cite métricas para regressão...\n",
            "Vetor criado com dimensão: (1, 384)\n",
            "Buscando 3 documentos de 5 disponíveis\n",
            "Distâncias encontradas: [ 8.156842 15.423008 16.215746]\n",
            "Índices encontrados: [3 1 4]\n",
            "Documento 3: As métricas servem para medir a qualidade dos modelos. Na classificação, usamos: precisão, recall, F...\n",
            "Documento 1: Um pipeline de Ciência de Dados tem várias etapas: coleta dos dados, limpeza, análise exploratória (...\n",
            "Documento 4: Deploy é colocar o modelo para funcionar com dados reais. MLOps é o conjunto de boas práticas para m...\n",
            "Retornando 3 documentos\n",
            "Base carregada: base_ciencia_dados (5 documentos)\n",
            "Buscando documentos para: Cite métricas para regressão...\n",
            "Vetor criado com dimensão: (1, 384)\n",
            "Buscando 3 documentos de 5 disponíveis\n",
            "Distâncias encontradas: [ 8.156842 15.423008 16.215746]\n",
            "Índices encontrados: [3 1 4]\n",
            "Documento 3: As métricas servem para medir a qualidade dos modelos. Na classificação, usamos: precisão, recall, F...\n",
            "Documento 1: Um pipeline de Ciência de Dados tem várias etapas: coleta dos dados, limpeza, análise exploratória (...\n",
            "Documento 4: Deploy é colocar o modelo para funcionar com dados reais. MLOps é o conjunto de boas práticas para m...\n",
            "Retornando 3 documentos\n",
            "Base carregada: base_ciencia_dados (5 documentos)\n",
            "Buscando documentos para: O que são embeddings?...\n",
            "Vetor criado com dimensão: (1, 384)\n",
            "Buscando 3 documentos de 5 disponíveis\n",
            "Distâncias encontradas: [13.299231 16.32703  16.96973 ]\n",
            "Índices encontrados: [2 1 4]\n",
            "Documento 2: Embeddings são formas de transformar textos em números. Com isso, o computador consegue comparar fra...\n",
            "Documento 1: Um pipeline de Ciência de Dados tem várias etapas: coleta dos dados, limpeza, análise exploratória (...\n",
            "Documento 4: Deploy é colocar o modelo para funcionar com dados reais. MLOps é o conjunto de boas práticas para m...\n",
            "Retornando 3 documentos\n"
          ]
        }
      ],
      "source": [
        "def criar_interface():\n",
        "    \"\"\"\n",
        "    Cria a interface web usando Gradio\n",
        "    \"\"\"\n",
        "    # Obter bases disponíveis\n",
        "    bases_disponiveis = listar_bases_disponiveis()\n",
        "\n",
        "    # Se não há bases, criar uma de exemplo\n",
        "    if not bases_disponiveis:\n",
        "        print(\"Nenhuma base encontrada. Criando base de exemplo...\")\n",
        "        if criar_base_exemplo():\n",
        "            bases_disponiveis = listar_bases_disponiveis()\n",
        "\n",
        "    with gr.Blocks(title=\"PrimeiroRAG Simplificado\") as interface:\n",
        "\n",
        "        # Título\n",
        "        gr.Markdown(\"# PrimeiroRAG - Sistema RAG Local\")\n",
        "        gr.Markdown(\"Sistema de perguntas e respostas usando modelos de IA locais\")\n",
        "\n",
        "        with gr.Row():\n",
        "            # Coluna de configurações\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Configurações\")\n",
        "\n",
        "                # Seleção da base\n",
        "                dropdown_base = gr.Dropdown(\n",
        "                    choices=bases_disponiveis,\n",
        "                    value=bases_disponiveis[0] if bases_disponiveis else None,\n",
        "                    label=\"Base de Conhecimento\",\n",
        "                    info=\"Selecione a base de dados\"\n",
        "                )\n",
        "\n",
        "                # Seleção do modelo (apenas visual, só temos um)\n",
        "                dropdown_modelo = gr.Dropdown(\n",
        "                    choices=[\"FLAN-T5 Base\"],\n",
        "                    value=\"FLAN-T5 Base\",\n",
        "                    label=\"Modelo de IA\",\n",
        "                    info=\"Modelo de linguagem local\"\n",
        "                )\n",
        "\n",
        "                # Botão para atualizar bases\n",
        "                botao_atualizar = gr.Button(\"Atualizar Bases\")\n",
        "\n",
        "            # Coluna principal do chat\n",
        "            with gr.Column(scale=3):\n",
        "                gr.Markdown(\"### Chat\")\n",
        "\n",
        "                # Interface do chat\n",
        "                chat_interface = gr.ChatInterface(\n",
        "                    fn=chat_principal,\n",
        "                    description=\"Faça perguntas sobre o conteúdo da base de conhecimento selecionada.\",\n",
        "                    type=\"messages\",\n",
        "                    additional_inputs=[dropdown_base, dropdown_modelo],\n",
        "                    submit_btn=\"Enviar\"\n",
        "                )\n",
        "\n",
        "        # Função para atualizar lista de bases\n",
        "        def atualizar_bases():\n",
        "            novas_bases = listar_bases_disponiveis()\n",
        "            return gr.Dropdown(choices=novas_bases)\n",
        "\n",
        "        # Conectar botão de atualizar\n",
        "        botao_atualizar.click(atualizar_bases, outputs=dropdown_base)\n",
        "\n",
        "    return interface\n",
        "\n",
        "\n",
        "### PARTE 9: Executar aplicação\n",
        "\n",
        "\n",
        "def executar_PrimeiroRAG():\n",
        "    \"\"\"\n",
        "    Executa o sistema PrimeiroRAG\n",
        "    \"\"\"\n",
        "    print(\"Iniciando PrimeiroRAG...\")\n",
        "\n",
        "    # Verificar se há bases disponíveis\n",
        "    bases = listar_bases_disponiveis()\n",
        "    print(f\"Bases disponíveis: {len(bases)}\")\n",
        "\n",
        "    if not bases:\n",
        "        print(\"Criando base de exemplo...\")\n",
        "        criar_base_exemplo()\n",
        "\n",
        "    # Criar e lançar interface\n",
        "    interface = criar_interface()\n",
        "\n",
        "    print(\"Lançando interface...\")\n",
        "    interface.launch(\n",
        "        share=True,\n",
        "        debug=True\n",
        "    )\n",
        "\n",
        "# Executar o sistema\n",
        "if __name__ == \"__main__\":\n",
        "    executar_PrimeiroRAG()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASqgY5HvKF6U"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "- Inclua mais uma base de conhecimento, chamada \"base_ciência_dados\" com textos sobre ciência de dados\n",
        "- Ao selecionar a nova base na caixa de seleção \"Base de Conhecimento - Selecione a base de dados\" da interface, o RAG deve responder perguntas sobre a base selecionada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ieh-TCM0AUhL"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"DB/\"\n",
        "\n",
        "def criar_base_ciencia_dados():\n",
        "    \"\"\"\n",
        "    Cria uma base de dados\n",
        "    \"\"\"\n",
        "    print(\"Criando base de dados sobre Ciência de Dados...\")\n",
        "\n",
        "    # Dados de exemplo\n",
        "    documentos = [\n",
        "        {\n",
        "            \"text\":\n",
        "                \"Ciência de Dados é a área que usa estatística, programação e conhecimento de negócio para analisar dados e gerar insights. \"\n",
        "                \"Ela ajuda empresas a entender padrões, prever situações e tomar decisões melhores.\"\n",
        "            ,\n",
        "            \"source\": \"Introdução à Ciência de Dados\"\n",
        "        },\n",
        "        {\n",
        "            \"text\":\n",
        "                \"Um pipeline de Ciência de Dados tem várias etapas: \"\n",
        "                \"coleta dos dados, limpeza, análise exploratória (EDA), criação de novas variáveis, escolha do modelo, avaliação e deployment. \"\n",
        "                \"Cada etapa ajuda a transformar dados brutos em soluções úteis.\"\n",
        "            ,\n",
        "            \"source\": \"Pipeline de Ciência de Dados\"\n",
        "        },\n",
        "        {\n",
        "            \"text\":\n",
        "                \"Embeddings são formas de transformar textos em números. \"\n",
        "                \"Com isso, o computador consegue comparar frases, buscar respostas parecidas e melhorar a precisão de modelos como o RAG. \"\n",
        "                \"Modelos como BERT e MiniLM são usados para gerar esses vetores numéricos.\"\n",
        "            ,\n",
        "            \"source\": \"Embeddings e Busca Semântica\"\n",
        "        },\n",
        "        {\n",
        "            \"text\":\n",
        "                \"As métricas servem para medir a qualidade dos modelos. \"\n",
        "                \"Na classificação, usamos: precisão, recall, F1-score e AUC-ROC. \"\n",
        "                \"Na regressão, usamos: MAE, RMSE e R². \"\n",
        "                \"No ranking, usamos: MAP e NDCG.\"\n",
        "            ,\n",
        "            \"source\": \"Métricas em Modelos de Machine Learning\"\n",
        "        },\n",
        "        {\n",
        "            \"text\":\n",
        "                \"Deploy é colocar o modelo para funcionar com dados reais. \"\n",
        "                \"MLOps é o conjunto de boas práticas para manter esse modelo estável. \"\n",
        "                \"Isso inclui: testes, monitoramento, alertas e re-treinamento quando o modelo começa a errar.\"\n",
        "            ,\n",
        "            \"source\": \"Boas Práticas de Deploy e MLOps\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Criar diretório\n",
        "        caminho_exemplo = os.path.join(BASE_DIR, \"base_ciencia_dados\")\n",
        "        os.makedirs(caminho_exemplo, exist_ok=True)\n",
        "\n",
        "        # Gerar embeddings\n",
        "        embeddings = []\n",
        "        for doc in documentos:\n",
        "            embedding = criar_embedding(doc[\"text\"])\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        # Criar índice FAISS\n",
        "        embeddings_array = np.array(embeddings).astype('float32')\n",
        "        dimensao = embeddings_array.shape[1]\n",
        "        indice = faiss.IndexFlatL2(dimensao)\n",
        "        indice.add(embeddings_array)\n",
        "\n",
        "        # Salvar arquivos\n",
        "        faiss.write_index(indice, os.path.join(caminho_exemplo, \"faiss_index.index\"))\n",
        "\n",
        "        with open(os.path.join(caminho_exemplo, \"chunk_metadata.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(documentos, f)\n",
        "\n",
        "        print(\"Base de exemplo criada com sucesso!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao criar base de exemplo: {e}\")\n",
        "\n",
        "criar_base_ciencia_dados()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}